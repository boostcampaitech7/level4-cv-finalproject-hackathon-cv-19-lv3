{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# warning!!!!!!!!!!!!!!\n",
    "- 본 노트북의 코드는 현재 업데이트된 streamlit 코드와 호환되지 않는 부분이 많으므로 혹시 해당 노트북에서 코드를 발췌하여 사용하지 않도록 주의해주세요!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "\n",
    "VideoFileClip('hong.gif').write_videofile('hong_test.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "# Setting up the Pose model for images.\n",
    "pose_img = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n",
    "# Setting up the Pose model for videos.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, \n",
    "                          min_tracking_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Initializing mediapipe drawing class to draw landmarks on specified image.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def estimPose_img(input_file, pose=pose_img, landmarks_c=(234,63,247), connection_c=(117,249,77), \n",
    "                   thickness=20, circle_r=10, display=True):\n",
    "    \n",
    "    # Read the input image\n",
    "    if isinstance(input_file, str) :\n",
    "        input_img = cv2.imread(input_file)\n",
    "    else :\n",
    "        input_img = input_file\n",
    "    \n",
    "    # Create a copy of the input image\n",
    "    output_img = input_img.copy()\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(output_img) # 이거 이렇게 하면 트래킹은 안되자나..\n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = input_img.shape\n",
    "    \n",
    "    # Initialize a list to store the detected landmarks.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Check if any landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "        skeleton = np.zeros_like(input_img)\n",
    "        mp_drawing.draw_landmarks(skeleton, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                                  mp_drawing.DrawingSpec(landmarks_c, thickness, circle_r),\n",
    "                                  mp_drawing.DrawingSpec(connection_c, thickness, circle_r))\n",
    "    \n",
    "        # Draw Pose landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(output_img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                                  mp_drawing.DrawingSpec(landmarks_c, thickness, circle_r),\n",
    "                                  mp_drawing.DrawingSpec(connection_c, thickness, circle_r))\n",
    "        \n",
    "        # Iterate over the detected landmarks.\n",
    "        for landmark in results.pose_world_landmarks.landmark:\n",
    "            landmarks.append((landmark.x, landmark.y,\n",
    "                                  landmark.z, landmark.visibility))\n",
    "            \n",
    "    # print(results.pose_landmarks)\n",
    "    # Check if we want to display.\n",
    "    if display:\n",
    "        # Display the original input image and the resulting image.\n",
    "        plt.figure(figsize=[15,15])\n",
    "        plt.subplot(121);plt.imshow(input_img[:,:,::-1]);plt.title(\"Original image\");plt.axis('off')\n",
    "        plt.subplot(122);plt.imshow(output_img[:,:,::-1]);plt.title(\"Output image\");plt.axis('off')\n",
    "        \n",
    "        # Plot the Pose landmarks in 3D.\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        return output_img, skeleton, landmarks\n",
    "        \n",
    "    # Just get output_img and landmarks\n",
    "    else:\n",
    "        # Return the output image and the found landmarks.\n",
    "        return output_img, skeleton, landmarks\n",
    "\n",
    "def estimPose_video(input_file, pose_video=pose_video, landmarks_c=(234,63,247), connection_c=(117,249,77), \n",
    "                 thickness=1, circle_r=1, nrows_frames=4, ncols_frames=3):\n",
    "    \n",
    "    # Initialize the VideoCapture object to read from a video stored in the disk.\n",
    "    video = cv2.VideoCapture(input_file)\n",
    "    \n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    original_video_frames = []\n",
    "    only_skeleton_frames = []\n",
    "    \n",
    "    all_landmarks = []\n",
    "    for i in range(total_frames):\n",
    "        # Read a frame.\n",
    "        ok, frame = video.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        original_video_frames.append(frame.copy())\n",
    "    \n",
    "        # Check if frame is not read properly.\n",
    "        if not ok:\n",
    "            # Break the loop.\n",
    "            break\n",
    "        \n",
    "        # Get the width and height of the frame\n",
    "        frame_height, frame_width, _ =  frame.shape\n",
    "        # Resize the frame while keeping the aspect ratio.\n",
    "        frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "        frame, skeleton, landmarks = estimPose_img(frame, pose_video, landmarks_c, connection_c, thickness, \n",
    "                              circle_r, display=False)\n",
    "        frames.append(frame)\n",
    "        all_landmarks.append(landmarks)\n",
    "        only_skeleton_frames.append(skeleton)\n",
    "    return original_video_frames, only_skeleton_frames, frames, all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_video_frames, only_skeleton_frames, frames, _ = estimPose_video(\"../hong.mp4\", thickness=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, C = frames[0].shape\n",
    "fps = 30\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n",
    "\n",
    "for frame in frames:\n",
    "    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "\n",
    "output_path = \"test.mp4\"\n",
    "iio.imwrite(output_path, frames, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image\n",
    "\n",
    "def get_detection(img_path):\n",
    "    base_options = python.BaseOptions(model_asset_path='../pose_landmarker.task')\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        output_segmentation_masks=True)\n",
    "    detector = vision.PoseLandmarker.create_from_options(options)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "    annotated_image = draw_landmarks_on_image(mp_image.numpy_view(), detection_result)\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.show()\n",
    "    \n",
    "    return detection_result.pose_landmarks, detection_result.segmentation_masks, annotated_image\n",
    "r, s, a = get_detection('../images/target_img.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_area(segmentation_masks, thr=0.5):\n",
    "    return np.sqrt(np.sum(segmentation_masks.numpy_view() > thr))\n",
    "\n",
    "def refine_landmarks(landmarks):\n",
    "    lst = []\n",
    "    for landmark in landmarks:\n",
    "        lst.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "    return np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_landmarks, target_masks, target_annotated_image = get_detection(\"../images/target_img.jpg\")\n",
    "target_landmarks_np = refine_landmarks(target_landmarks[0])\n",
    "target_area = get_area(target_masks[0])\n",
    "\n",
    "right_landmarks, right_masks, right_annotated_image = get_detection(\"../images/right_pose_img.jpg\")\n",
    "right_landmarks_np = refine_landmarks(right_landmarks[0])\n",
    "right_area = get_area(right_masks[0])\n",
    "\n",
    "wrong_landmarks, wrong_masks, wrong_annotated_image = get_detection(\"../images/wrong_pose_img.jpg\")\n",
    "wrong_landmarks_np = refine_landmarks(wrong_landmarks[0])\n",
    "wrong_area = get_area(wrong_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypoint_map import *\n",
    "from scoring import normalize_landmarks\n",
    "\n",
    "normalized_target_landmarks_np = normalize_landmarks(target_landmarks_np)\n",
    "normalized_right_landmarks_np =  normalize_landmarks(right_landmarks_np)\n",
    "normalized_wrong_landmarks_np =  normalize_landmarks(wrong_landmarks_np)\n",
    "\n",
    "# normalized_target_landmarks_np = filter_important_features(normalize_landmarks(target_landmarks_np))\n",
    "# normalized_right_landmarks_np = filter_important_features(normalize_landmarks(right_landmarks_np))\n",
    "# normalized_wrong_landmarks_np = filter_important_features(normalize_landmarks(wrong_landmarks_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT가 알려준 유클리드 거리 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(landmarks1, landmarks2):\n",
    "    if landmarks1 is None or landmarks2 is None:\n",
    "        return 0  # 비교 불가 시 유사성 0으로 처리\n",
    "    \n",
    "    if landmarks1.shape != landmarks2.shape:\n",
    "        raise ValueError(\"both landmarks must have same shape!!\")\n",
    "    \n",
    "    if landmarks1.shape[-1] == 4:\n",
    "        landmarks1 = landmarks1[..., :3]\n",
    "        landmarks2 = landmarks2[..., :3]\n",
    "    # 평준화된 유클리드 거리 계산\n",
    "    distance = np.linalg.norm(landmarks1 - landmarks2, axis=1)\n",
    "    similarity = 1 / (1 + np.mean(distance))  # 유사성을 0~1로 정규화\n",
    "    return similarity\n",
    "\n",
    "print(\"target and right pose L1 score : \", calculate_similarity(normalized_target_landmarks_np, normalized_right_landmarks_np))\n",
    "print(\"target and wrong pose L1 score : \", calculate_similarity(normalized_target_landmarks_np, normalized_wrong_landmarks_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(landmarks1, landmarks2):\n",
    "    if landmarks1.shape != landmarks2.shape:\n",
    "        raise ValueError(\"both landmarks must have same shape!!\")\n",
    "    \n",
    "    if landmarks1.shape[-1] == 4:\n",
    "        landmarks1 = landmarks1[..., :3]\n",
    "        landmarks2 = landmarks2[..., :3]\n",
    "    landmarks1 = landmarks1.flatten()\n",
    "    landmarks2 = landmarks2.flatten()\n",
    "    return (1+np.dot(landmarks1, landmarks2)/(np.linalg.norm(landmarks1)*np.linalg.norm(landmarks2))) / 2\n",
    "\n",
    "print(f\"target and right pose sim : {cos_sim(normalized_target_landmarks_np, normalized_right_landmarks_np)}\")\n",
    "print(f\"target and wrong pose sim : {cos_sim(normalized_target_landmarks_np, normalized_wrong_landmarks_np)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"target and right pose sim : {cos_sim(target_landmarks_np, right_landmarks_np)}\")\n",
    "print(f\"target and wrong pose sim : {cos_sim(target_landmarks_np, wrong_landmarks_np)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target and right pose L2 distance : \", np.linalg.norm((normalized_target_landmarks_np - normalized_right_landmarks_np)[..., :3].flatten()))\n",
    "print(\"target and wrong pose L2 distance : \", np.linalg.norm((normalized_target_landmarks_np - normalized_wrong_landmarks_np)[..., :3].flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target and right pose L1 distance : \", np.linalg.norm((normalized_target_landmarks_np - normalized_right_landmarks_np)[..., :3].flatten(), ord=1))\n",
    "print(\"target and wrong pose L1 distance : \", np.linalg.norm((normalized_target_landmarks_np - normalized_wrong_landmarks_np)[..., :3].flatten(), ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypoint_map import WEIGHTS, KEYPOINT_MAPPING\n",
    "def weighted_distance(A, B, weights = WEIGHTS):\n",
    "    d = 0.0\n",
    "    \n",
    "    for i in range(NUM_CLASSES):\n",
    "        d += weights[KEYPOINT_MAPPING[i]] * np.linalg.norm(A[i] - B[i], ord=1)\n",
    "    return d\n",
    "\n",
    "print(\"target and right pose weighted L1 distance : \", weighted_distance(normalized_target_landmarks_np[..., :3], normalized_right_landmarks_np[..., :3]))\n",
    "print(\"target and wrong pose weighted L1 distance : \", weighted_distance(normalized_target_landmarks_np[..., :3], normalized_wrong_landmarks_np[..., :3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='pose_landmarker.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO)\n",
    "\n",
    "def get_video_detection(video_path, landmarks_c=(234,63,247), connection_c=(117,249,77), \n",
    "                   thickness=20, circle_r=10):\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "        frame_duration = int(1000 / fps)\n",
    "        frames = []\n",
    "        landmarks = []\n",
    "\n",
    "        for i in range(total_frames):\n",
    "            # Read a frame.\n",
    "            ok, frame = video.read()\n",
    "            frame_timestamp_ms = i * frame_duration\n",
    "\n",
    "            # Check if frame is not read properly.\n",
    "            if not ok:\n",
    "                # Break the loop.\n",
    "                break\n",
    "\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "            frames.append(draw_landmarks_on_image(mp_image.numpy_view(), pose_landmarker_result))\n",
    "            landmarks.append(pose_landmarker_result.pose_landmarks[0])\n",
    "    return frames, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara_1_frames, mara_1_landmarks = get_video_detection(\"../마라탕후루1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara_2_frames, mara_2_landmarks = get_video_detection(\"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_frames, enter_landmarks = get_video_detection(\"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(mara_1_frames), len(mara_2_frames), len(enter_frames))\n",
    "\n",
    "refined_mara_1_landmarks = [refine_landmarks(landmark) for landmark in mara_1_landmarks][:min_length]\n",
    "refined_mara_2_landmarks = [refine_landmarks(landmark) for landmark in mara_2_landmarks][:min_length]\n",
    "refined_enter_landmarks = [refine_landmarks(landmark) for landmark in enter_landmarks][:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mara_1_landmarks = [normalize_landmarks(landmark) for landmark in refined_mara_1_landmarks]\n",
    "normalized_mara_2_landmarks = [normalize_landmarks(landmark) for landmark in refined_mara_2_landmarks]\n",
    "normalized_enter_landmarks = [normalize_landmarks(landmark) for landmark in refined_enter_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara_1_2_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(normalized_mara_1_landmarks, normalized_mara_2_landmarks)])\n",
    "mara_1_enter_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(normalized_mara_1_landmarks, normalized_enter_landmarks)])\n",
    "mara_2_enter_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(normalized_mara_2_landmarks, normalized_enter_landmarks)])\n",
    "\n",
    "print(f\"target and right pose sim : {mara_1_2_sim}\")\n",
    "print(f\"target and wrong pose sim : {mara_1_enter_sim}\")\n",
    "print(f\"target and wrong pose sim_2 : {mara_2_enter_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara_1_2_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(refined_mara_1_landmarks, refined_mara_2_landmarks)])\n",
    "mara_1_enter_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(refined_mara_1_landmarks, refined_enter_landmarks)])\n",
    "mara_2_enter_sim = np.mean([cos_sim(lm1, lm2) for lm1, lm2 in zip(refined_mara_2_landmarks, refined_enter_landmarks)])\n",
    "\n",
    "print(f\"target and right pose sim : {mara_1_2_sim}\")\n",
    "print(f\"target and wrong pose sim : {mara_1_enter_sim}\")\n",
    "print(f\"target and wrong pose sim_2 : {mara_2_enter_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target and right pose L2 norm: \", \n",
    "    np.mean([np.linalg.norm(landmarks1[..., :3] - landmarks2[..., :3])\n",
    "    for landmarks1, landmarks2 in zip(normalized_mara_1_landmarks, normalized_mara_2_landmarks)]))\n",
    "print(\"target and wrong pose L2 norm: \", \n",
    "    np.mean([np.linalg.norm(landmarks1[..., :3] - landmarks2[..., :3])\n",
    "    for landmarks1, landmarks2 in zip(normalized_mara_1_landmarks, normalized_enter_landmarks)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(landmarks1, landmarks2):\n",
    "    if landmarks1 is None or landmarks2 is None:\n",
    "        return 0  # 비교 불가 시 유사성 0으로 처리\n",
    "    \n",
    "    if landmarks1.shape != landmarks2.shape:\n",
    "        raise ValueError(\"both landmarks must have same shape!!\")\n",
    "    \n",
    "    if landmarks1.shape[-1] == 4:\n",
    "        landmarks1 = landmarks1[..., :3]\n",
    "        landmarks2 = landmarks2[..., :3]\n",
    "    # 평준화된 유클리드 거리 계산\n",
    "    distance = np.linalg.norm(landmarks1 - landmarks2, axis=1)\n",
    "    similarity = 1 / (1 + np.mean(distance))  # 유사성을 0~1로 정규화\n",
    "    return similarity\n",
    "\n",
    "print(\"target and right pose L2 norm: \", \n",
    "    np.mean([calculate_similarity(landmarks1[..., :3], landmarks2[..., :3])\n",
    "    for landmarks1, landmarks2 in zip(normalized_mara_1_landmarks, normalized_mara_2_landmarks)]))\n",
    "print(\"target and wrong pose L2 norm: \", \n",
    "    np.mean([calculate_similarity(landmarks1[..., :3], landmarks2[..., :3])\n",
    "    for landmarks1, landmarks2 in zip(normalized_mara_1_landmarks, normalized_enter_landmarks)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skeleton vector활용 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "# select for oks calculation\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "# for cosine similarity\n",
    "vector_list = [\n",
    "    (1, 2),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "    (6, 8),\n",
    "    (9, 11),\n",
    "    (10, 12),\n",
    "    (11, 13),\n",
    "    (12, 14)\n",
    "]\n",
    "\n",
    "\n",
    "# Get keypoints data & bounded box size from 1 frame\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    # return \n",
    "    # keypoints : list([x, y, z, visibility], ...)\n",
    "    # boxsize : detection box length[가로, 세로, 높이]\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "            \n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin, landmark.y), max(ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "    \n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# Calculate OKS value from 2 keypoints data from each data\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c**2)))\n",
    "\n",
    "\n",
    "# Make cosine similarity to percent form\n",
    "def cosine_similarity_to_percentage(similarity_list):\n",
    "    similarity = np.mean(similarity_list)\n",
    "    return (similarity + 1) * 50\n",
    "\n",
    "\n",
    "# Calculate cosine similarity from each keypoint data\n",
    "def cos_sim_w_keypoint(keypoints1, keypoints2):\n",
    "    global vector_list\n",
    "    cos_sim_list = []\n",
    "\n",
    "    for vector in vector_list:\n",
    "        z_num = 2\n",
    "        idx1, idx2 = vector\n",
    "        vec1 = (keypoints1[idx2][:z_num] - keypoints1[idx1][:z_num]).reshape(1, -1)\n",
    "        vec2 = (keypoints2[idx2][:z_num] - keypoints2[idx1][:z_num]).reshape(1, -1)\n",
    "        sim_value = cosine_similarity(vec1, vec2)\n",
    "        cos_sim_list.append(sim_value)\n",
    "    \n",
    "    return cos_sim_list\n",
    "\n",
    "\n",
    "# Calculate OKS & Cosine similarity from each keypoint data\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "    keypoints1 = np.array(keypoints1)\n",
    "    keypoints2 = np.array(keypoints2)\n",
    "\n",
    "    if keypoints1.shape != keypoints2.shape:\n",
    "        print(keypoints1.shape, keypoints2.shape)\n",
    "        raise ValueError(\"Keypoint shapes do not match!\")\n",
    "    \n",
    "    oks_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "\n",
    "    cos_sim_list = cos_sim_w_keypoint(keypoints1, keypoints2)\n",
    "\n",
    "    return cosine_similarity_to_percentage(np.mean(cos_sim_list)), (np.mean(oks_list)) * 100\n",
    "\n",
    "\n",
    "# Make mean coordinate data from keypoints list\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    frame_count = -1\n",
    "\n",
    "    # List of OKS & Cosine similarity from each frame\n",
    "    okslist = []\n",
    "    cos_list = []\n",
    "\n",
    "    # List of OKS & Cosine similarity from every 15 frame\n",
    "    okslist_mean = []\n",
    "    cos_list_mean = []\n",
    "\n",
    "    # Make keypoint list\n",
    "    list_keypoints1 = []\n",
    "    list_keypoints2 = []\n",
    "\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        frame_count += 1\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            keypoints1, boxsize = get_keypoints_and_boxsize(frame1)\n",
    "            keypoints2, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            list_keypoints1.append(keypoints1)\n",
    "            list_keypoints2.append(keypoints2)\n",
    "\n",
    "            similarity, oks_percent = weighted_similarity(keypoints1, keypoints2, boxsize) # Calculate Scores from each frame\n",
    "            okslist.append(oks_percent)\n",
    "            cos_list.append(similarity)\n",
    "            print(f\"Frame {frame_count+1}: Weighted similarity between keypoints1 and video: {similarity}\")\n",
    "            print(f\"Frame {frame_count+1}: Weighted similarity between keypoints1 and video: {oks_percent}\")\n",
    "\n",
    "            if len(list_keypoints1) == 15:\n",
    "                mean_keypoints1 = mean_value_of_keypoints(list_keypoints1)\n",
    "                mean_keypoints2 = mean_value_of_keypoints(list_keypoints2)\n",
    "\n",
    "                similarity_mean, oks_percent_mean = weighted_similarity(mean_keypoints1, mean_keypoints2, boxsize) # Calculate Scores from each mean frame\n",
    "                okslist_mean.append(oks_percent_mean)\n",
    "                cos_list_mean.append(similarity_mean)\n",
    "                print(f\"Frame {frame_count+1}: Weighted similarity between mean keypoints1 and video: {similarity_mean}\")\n",
    "                print(f\"Frame {frame_count+1}: Weighted similarity between mean keypoints1 and video: {oks_percent_mean}\")\n",
    "\n",
    "                list_keypoints1 = []\n",
    "                list_keypoints2 = []\n",
    "            \n",
    "            # Press 'q' to exit the loop and close the video window\n",
    "            if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f'oks = {np.mean(okslist)}, cos = {np.mean(cos_list)}')           # Print the score from each frame\n",
    "    print(f'oks = {np.mean(okslist_mean)}, cos = {np.mean(cos_list_mean)}') # Print the score from every 15 frame\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../엔터테이먼트.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKS(거리기반 스코어), PCK(거리기반 정확도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Mediapipe 라이브러리를 임포트합니다.\n",
    "import numpy as np # 배열 및 수학 연산을 위한 NumPy 라이브러리를 임포트합니다.\n",
    "import cv2 # OpenCV 라이브러리를 임포트합니다.\n",
    "import shutil # 파일 복사 및 삭제를 위한 shutil 모듈을 임포트합니다.\n",
    "import os # 운영 체제 관련 작업을 위한 os 모듈을 임포트합니다.\n",
    "import json # JSON 데이터 처리를 위한 모듈을 임포트합니다.\n",
    "\n",
    "\n",
    "# Mediapipe 라이브러리를 이용한 포즈 추출 설정\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 사용할 키포인트 이름 및 선택된 키포인트 인덱스 목록 설정\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "oks_cnt = [[] for _ in range(11)]\n",
    "pck_cnt = [[] for _ in range(11)]\n",
    "\n",
    "def delete_file_or_folder(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                print(f\"File {path} deleted successfully.\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"Folder {path} and its contents deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Path {path} not found. Skipping deletion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting: {e}\")\n",
    "\n",
    "\n",
    "# 정답 프레임에서 키포인트 데이터 및 바운딩 박스 크기를 가져오는 함수\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin,\n",
    "                                                                                                       landmark.y), max(\n",
    "                    ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "\n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# OKS 값 계산 함수\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c ** 2)))\n",
    "\n",
    "\n",
    "# PCK 값 계산 함수\n",
    "def pck(gt, preds, threshold):\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    return 1.0 if distance < threshold else 0.0\n",
    "\n",
    "\n",
    "# 가중치가 적용된 유사도 계산 함수\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "        pck_list.append(pck(keypoints1[i][:3], keypoints2[i][:3], 0.1))\n",
    "\n",
    "    return (np.mean(oks_list)) * 100, (np.mean(pck_list)) * 100\n",
    "\n",
    "\n",
    "# 키포인트 리스트의 평균 좌표 계산 함수\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    frame_cnt = 0\n",
    "    # 업로드된 동영상 파일을 열기\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    # 각 프레임에서의 OKS 및 pck의 리스트\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "\n",
    "    # 사용자의 키포인트 리스트\n",
    "    user_keypoints = []\n",
    "\n",
    "    # 동영상의 모든 프레임을 처리\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        ret1, frame1 = cap1.read()\n",
    "        frame1 = cv2.flip(frame1, 1)\n",
    "\n",
    "        ret2, frame2 = cap2.read()\n",
    "        frame2 = cv2.flip(frame2, 1)\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            # 현재 프레임에서 사용자의 키포인트 및 바운딩 박스 크기 가져오기\n",
    "            user_key, _ = get_keypoints_and_boxsize(frame1)\n",
    "            user_keypoints.append(user_key)\n",
    "\n",
    "            answer_key, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            # 만약 정답 키포인트와 사용자 키포인트가 존재하면 점수 계산\n",
    "            if len(answer_key) > 0 and len(user_key) > 0:\n",
    "                oks_percent, pck_percent = weighted_similarity(np.array(answer_key), np.array(user_key),\n",
    "                                                               _)  # Calculate Scores from each frame\n",
    "                oks_cnt[int(oks_percent / 10)].append(frame_cnt)\n",
    "                pck_cnt[int(pck_percent / 10)].append(frame_cnt)\n",
    "\n",
    "                oks_list.append(oks_percent)\n",
    "                pck_list.append(pck_percent)\n",
    "        else:\n",
    "            break\n",
    "        frame_cnt = frame_cnt + 1\n",
    "\n",
    "    oks_answer = np.mean(oks_list)\n",
    "    pck_answer = np.mean(pck_list)\n",
    "    print(\"oks =\", oks_answer, \"pck =\", pck_answer)\n",
    "\n",
    "    # JSON 응답에 넣을 데이터를 딕셔너리로 만듦 (값들을 float로 변환)\n",
    "    response_data = {\n",
    "        \"oks_30\": oks_answer,\n",
    "        \"pck_30\": pck_answer,\n",
    "        \"oks_frame_score\": oks_list,\n",
    "        \"pck_frame_score\": pck_list\n",
    "    }\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks, _ = get_keypoints_and_boxsize(cv2.imread('../images/right_pose_img.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(landmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루2.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거리기반 OKS, PCK and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Mediapipe 라이브러리를 임포트합니다.\n",
    "import numpy as np # 배열 및 수학 연산을 위한 NumPy 라이브러리를 임포트합니다.\n",
    "import cv2 # OpenCV 라이브러리를 임포트합니다.\n",
    "import shutil # 파일 복사 및 삭제를 위한 shutil 모듈을 임포트합니다.\n",
    "import os # 운영 체제 관련 작업을 위한 os 모듈을 임포트합니다.\n",
    "import json # JSON 데이터 처리를 위한 모듈을 임포트합니다.\n",
    "\n",
    "\n",
    "# Mediapipe 라이브러리를 이용한 포즈 추출 설정\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 사용할 키포인트 이름 및 선택된 키포인트 인덱스 목록 설정\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "oks_cnt = [[] for _ in range(11)]\n",
    "pck_cnt = [[] for _ in range(11)]\n",
    "\n",
    "def delete_file_or_folder(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                print(f\"File {path} deleted successfully.\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"Folder {path} and its contents deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Path {path} not found. Skipping deletion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting: {e}\")\n",
    "\n",
    "\n",
    "# 정답 프레임에서 키포인트 데이터 및 바운딩 박스 크기를 가져오는 함수\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin,\n",
    "                                                                                                       landmark.y), max(\n",
    "                    ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "\n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# OKS 값 계산 함수\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c ** 2)))\n",
    "\n",
    "\n",
    "# PCK 값 계산 함수\n",
    "def pck(gt, preds, threshold):\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    return 1.0 if distance < threshold else 0.0\n",
    "\n",
    "\n",
    "# 가중치가 적용된 유사도 계산 함수\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "        pck_list.append(pck(keypoints1[i][:3], keypoints2[i][:3], 0.1))\n",
    "\n",
    "    return (np.mean(oks_list)) * 100, (np.mean(pck_list)) * 100\n",
    "\n",
    "\n",
    "# 키포인트 리스트의 평균 좌표 계산 함수\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    frame_cnt = 0\n",
    "    # 업로드된 동영상 파일을 열기\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    # 각 프레임에서의 OKS 및 pck의 리스트\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "\n",
    "    # 사용자의 키포인트 리스트\n",
    "    user_keypoints = []\n",
    "\n",
    "    # 동영상의 모든 프레임을 처리\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        ret1, frame1 = cap1.read()\n",
    "        frame1 = cv2.flip(frame1, 1)\n",
    "\n",
    "        ret2, frame2 = cap2.read()\n",
    "        frame2 = cv2.flip(frame2, 1)\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            # 현재 프레임에서 사용자의 키포인트 및 바운딩 박스 크기 가져오기\n",
    "            user_key, _ = get_keypoints_and_boxsize(frame1)\n",
    "            user_keypoints.append(user_key)\n",
    "\n",
    "            answer_key, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            # 만약 정답 키포인트와 사용자 키포인트가 존재하면 점수 계산\n",
    "            if len(answer_key) > 0 and len(user_key) > 0:\n",
    "                oks_percent, pck_percent = weighted_similarity(np.array(answer_key), np.array(user_key),\n",
    "                                                               _)  # Calculate Scores from each frame\n",
    "                oks_cnt[int(oks_percent / 10)].append(frame_cnt)\n",
    "                pck_cnt[int(pck_percent / 10)].append(frame_cnt)\n",
    "\n",
    "                oks_list.append(oks_percent)\n",
    "                pck_list.append(pck_percent)\n",
    "        else:\n",
    "            break\n",
    "        frame_cnt = frame_cnt + 1\n",
    "\n",
    "    oks_answer = np.mean(oks_list)\n",
    "    pck_answer = np.mean(pck_list)\n",
    "    print(\"oks =\", oks_answer, \"pck =\", pck_answer)\n",
    "\n",
    "    # JSON 응답에 넣을 데이터를 딕셔너리로 만듦 (값들을 float로 변환)\n",
    "    response_data = {\n",
    "        \"oks_30\": oks_answer,\n",
    "        \"pck_30\": pck_answer,\n",
    "        \"oks_frame_score\": oks_list,\n",
    "        \"pck_frame_score\": pck_list\n",
    "    }\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737366229.695682   86745 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1737366229.712817   91261 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (Intel(R) Arc(TM) Graphics)\n",
      "W0000 00:00:1737366230.289546   91264 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737366231.197976   91279 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1737366231.650636   86745 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1737366231.661252   91287 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (Intel(R) Arc(TM) Graphics)\n",
      "W0000 00:00:1737366232.428957   91288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737366233.372087   91300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference of keypoint nose: 0.1570171220940648\n",
      "difference of keypoint left_ear: 0.2413271117898635\n",
      "difference of keypoint right_ear: 0.1650120571159327\n",
      "difference of keypoint left_shoulder: 0.2583203266022461\n",
      "difference of keypoint right_shoulder: 0.12468798032980599\n",
      "difference of keypoint left_elbow: 0.27143862948163006\n",
      "difference of keypoint right_elbow: 0.18094223753063535\n",
      "difference of keypoint left_wrist: 0.07772784161598169\n",
      "difference of keypoint right_wrist: 0.5088597423511898\n",
      "difference of keypoint left_pinky: 0.08009241116378073\n",
      "difference of keypoint right_pinky: 0.5107978513783973\n",
      "difference of keypoint left_hip: 0.06811835572471417\n",
      "difference of keypoint right_hip: 0.13021131333086922\n",
      "difference of keypoint left_knee: 0.07405856650211821\n",
      "difference of keypoint right_knee: 0.15767739230719596\n",
      "difference of keypoint left_ankle: 0.06921558740869072\n",
      "difference of keypoint right_ankle: 0.15099166465880529\n",
      "difference of keypoint left_heel: 0.27145726639599166\n",
      "difference of keypoint right_heel: 0.3372374991502642\n",
      "difference of keypoint left_foot_index: 0.18547924833518414\n",
      "difference of keypoint right_foot_index: 0.23778566264262818\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mediapipe_inference import get_detection\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scoring import refine_landmarks\n",
    "\n",
    "\n",
    "def normalize_landmarks_to_range(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Normalize landmarks2 to match the coordinate range of landmarks1.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (numpy array): Keypoints array for the first pose (num_selected_point, 4).\n",
    "        landmarks2 (numpy array): Keypoints array for the second pose (num_selected_point, 4).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Normalized landmarks2 matching the range of landmarks1.\n",
    "    \"\"\"\n",
    "    # Calculate min and max for landmarks1 and landmarks2\n",
    "    min1 = np.min(landmarks1[:, :3], axis=0)  # (x_min, y_min, z_min) for landmarks1\n",
    "    max1 = np.max(landmarks1[:, :3], axis=0)  # (x_max, y_max, z_max) for landmarks1\n",
    "\n",
    "    min2 = np.min(landmarks2[:, :3], axis=0)  # (x_min, y_min, z_min) for landmarks2\n",
    "    max2 = np.max(landmarks2[:, :3], axis=0)  # (x_max, y_max, z_max) for landmarks2\n",
    "\n",
    "    # Normalize landmarks2 to the range of landmarks1\n",
    "    normalized_landmarks2 = (landmarks2[:, :3] - min2) / (max2 - min2) * (max1 - min1) + min1\n",
    "\n",
    "    # Combine normalized coordinates with the original visibility values\n",
    "    normalized_landmarks2 = np.hstack((normalized_landmarks2, landmarks2[:, 3:4]))\n",
    "\n",
    "    return normalized_landmarks2\n",
    "\n",
    "\n",
    "def draw_landmarks_on_blank_image(landmarks1, landmarks2, image_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Draw two sets of landmarks on a blank image.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (numpy array): Keypoints array for the first pose (num_selected_point, 4).\n",
    "        landmarks2 (numpy array): Keypoints array for the second pose (num_selected_point, 4).\n",
    "        image_size (tuple): Size of the blank image (height, width).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Image with landmarks visualized.\n",
    "    \"\"\"\n",
    "    # Create a blank image\n",
    "    blank_image = np.zeros((image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Normalize coordinates to fit within the image size\n",
    "    def normalize_coordinates(landmarks, image_size):\n",
    "        height, width = image_size\n",
    "        normalized_landmarks = []\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark[0] * width), int(landmark[1] * height)\n",
    "            normalized_landmarks.append((x, y))\n",
    "        return normalized_landmarks\n",
    "\n",
    "    # Normalize landmarks\n",
    "    normalized_landmarks1 = normalize_coordinates(landmarks1, image_size)\n",
    "    normalized_landmarks2 = normalize_coordinates(landmarks2, image_size)\n",
    "\n",
    "    # Draw landmarks1 (red)\n",
    "    for x, y in normalized_landmarks1:\n",
    "        cv2.circle(blank_image, (x, y), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    # Draw landmarks2 (blue)\n",
    "    for x, y in normalized_landmarks2:\n",
    "        cv2.circle(blank_image, (x, y), radius=5, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "    # Optionally connect landmarks (example: skeletal connections can be added here)\n",
    "    # You can define a list of connections (e.g., POSE_CONNECTIONS) to draw lines between keypoints.\n",
    "\n",
    "    return blank_image\n",
    "\n",
    "p1 = \"../images/승윤팔짱1.jpg\"\n",
    "p2= \"../images/준일팔짱.jpg\"\n",
    "l1, seg1, ann_img1, bs1 = get_detection(p1)\n",
    "l2, seg2, ann_img2, bs2 = get_detection(p2)\n",
    "np_l1 = refine_landmarks(l1)\n",
    "np_l2 = refine_landmarks(l2)\n",
    "np_l2 = normalize_landmarks_to_range(np_l1, np_l2)\n",
    "\n",
    "from keypoint_map import KEYPOINT_MAPPING, SELECTED_KEYPOINTS\n",
    "for i in range(np_l1.shape[0]):\n",
    "    print(f\"difference of keypoint {KEYPOINT_MAPPING[SELECTED_KEYPOINTS[i]]}: {np.linalg.norm(np_l2[i] - np_l1[i])}\")\n",
    "\n",
    "plt.imshow(draw_landmarks_on_blank_image(np_l1, np_l2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
