{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mp.solutions.pose를 활용한 구현\n",
    "- 주의점. 해당 파츠를 실행 시 이유는 모르겠지만, detector를 통한 구현이 경로 오류가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "# Setting up the Pose model for images.\n",
    "pose_img = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n",
    "# Setting up the Pose model for videos.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, \n",
    "                          min_tracking_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Initializing mediapipe drawing class to draw landmarks on specified image.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def estimPose_img(input_file, pose=pose_img, landmarks_c=(234,63,247), connection_c=(117,249,77), \n",
    "                   thickness=20, circle_r=10, display=True):\n",
    "    \n",
    "    # Read the input image\n",
    "    if isinstance(input_file, str) :\n",
    "        input_img = cv2.imread(input_file)\n",
    "    else :\n",
    "        input_img = input_file\n",
    "    \n",
    "    # Create a copy of the input image\n",
    "    output_img = input_img.copy()\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(output_img) # 이거 이렇게 하면 트래킹은 안되자나..\n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = input_img.shape\n",
    "    \n",
    "    # Initialize a list to store the detected landmarks.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Check if any landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "        skeleton = np.zeros_like(input_img)\n",
    "        mp_drawing.draw_landmarks(skeleton, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                                  mp_drawing.DrawingSpec(landmarks_c, thickness, circle_r),\n",
    "                                  mp_drawing.DrawingSpec(connection_c, thickness, circle_r))\n",
    "    \n",
    "        # Draw Pose landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(output_img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                                  mp_drawing.DrawingSpec(landmarks_c, thickness, circle_r),\n",
    "                                  mp_drawing.DrawingSpec(connection_c, thickness, circle_r))\n",
    "        \n",
    "        # Iterate over the detected landmarks.\n",
    "        for landmark in results.pose_world_landmarks.landmark:\n",
    "            landmarks.append((landmark.x, landmark.y,\n",
    "                                  landmark.z, landmark.visibility))\n",
    "            \n",
    "    # print(results.pose_landmarks)\n",
    "    # Check if we want to display.\n",
    "    if display:\n",
    "        # Display the original input image and the resulting image.\n",
    "        plt.figure(figsize=[15,15])\n",
    "        plt.subplot(121);plt.imshow(input_img[:,:,::-1]);plt.title(\"Original image\");plt.axis('off')\n",
    "        plt.subplot(122);plt.imshow(output_img[:,:,::-1]);plt.title(\"Output image\");plt.axis('off')\n",
    "        \n",
    "        # Plot the Pose landmarks in 3D.\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        return output_img, skeleton, landmarks\n",
    "        \n",
    "    # Just get output_img and landmarks\n",
    "    else:\n",
    "        # Return the output image and the found landmarks.\n",
    "        return output_img, skeleton, landmarks\n",
    "\n",
    "def estimPose_video(input_file, pose_video=pose_video, landmarks_c=(234,63,247), connection_c=(117,249,77), \n",
    "                 thickness=1, circle_r=1, nrows_frames=4, ncols_frames=3):\n",
    "    \n",
    "    # Initialize the VideoCapture object to read from a video stored in the disk.\n",
    "    video = cv2.VideoCapture(input_file)\n",
    "    \n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    original_video_frames = []\n",
    "    only_skeleton_frames = []\n",
    "    \n",
    "    all_landmarks = []\n",
    "    for i in range(total_frames):\n",
    "        # Read a frame.\n",
    "        ok, frame = video.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        original_video_frames.append(frame.copy())\n",
    "    \n",
    "        # Check if frame is not read properly.\n",
    "        if not ok:\n",
    "            # Break the loop.\n",
    "            break\n",
    "        \n",
    "        # Get the width and height of the frame\n",
    "        frame_height, frame_width, _ =  frame.shape\n",
    "        # Resize the frame while keeping the aspect ratio.\n",
    "        frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "        frame, skeleton, landmarks = estimPose_img(frame, pose_video, landmarks_c, connection_c, thickness, \n",
    "                              circle_r, display=False)\n",
    "        frames.append(frame)\n",
    "        all_landmarks.append(landmarks)\n",
    "        only_skeleton_frames.append(skeleton)\n",
    "    return original_video_frames, only_skeleton_frames, frames, all_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\naver_boostcamp\\project\\level4-cv-finalproject-hackathon-cv-19-lv3\\streamlit\\models\\pose_landmarker_heavy.task 파일이 이미 존재합니다.\n",
      "normalized result: \n",
      "L1_score: 0.899100075331185\n",
      "L2_distance: 0.6153559122962446\n",
      "cos_similarity: 0.9820466798958206\n",
      "PCK(thres=0.10): 0.5238095238095238\n",
      "oks:: 0.725824407828108\n",
      "matched: {'nose': False, 'left_ear': True, 'right_ear': False, 'left_shoulder': True, 'right_shoulder': False, 'left_elbow': False, 'right_elbow': False, 'left_wrist': False, 'right_wrist': True, 'left_pinky': False, 'right_pinky': False, 'left_hip': True, 'right_hip': False, 'left_knee': False, 'right_knee': True, 'left_ankle': True, 'right_ankle': True, 'left_heel': True, 'right_heel': True, 'left_foot_index': True, 'right_foot_index': True}\n",
      "\n",
      "\n",
      "\n",
      "not normalized result: \n",
      "L1_score: 0.7728069701652974\n",
      "L2_distance: 1.5183486853623762\n",
      "cos_similarity: 0.9792274390460615\n",
      "PCK(thres=0.10): 0.047619047619047616\n",
      "oks:: 0.3294177896409596\n",
      "matched: {'nose': False, 'left_ear': False, 'right_ear': False, 'left_shoulder': False, 'right_shoulder': False, 'left_elbow': True, 'right_elbow': False, 'left_wrist': False, 'right_wrist': False, 'left_pinky': False, 'right_pinky': False, 'left_hip': False, 'right_hip': False, 'left_knee': False, 'right_knee': False, 'left_ankle': False, 'right_ankle': False, 'left_heel': False, 'right_heel': False, 'left_foot_index': False, 'right_foot_index': False}\n"
     ]
    }
   ],
   "source": [
    "from scoring import evaluate_everything, refine_landmarks\n",
    "from detector import PoseDetector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 비교할 이미지 경로 설정\n",
    "p1 = \"../images/jun_v.jpg\"\n",
    "p2 = \"../images/wrong_pose_img.jpg\"\n",
    "\n",
    "# detector 정의. model_size로 크기 조정(0 ~ 2)\n",
    "image_detector = PoseDetector(model_size=2, mode='IMAGE')\n",
    "\n",
    "# 예측 수행\n",
    "l1, seg1, ann_img1, bs1 = image_detector.get_detection(p1)\n",
    "l2, seg2, ann_img2, bs2 = image_detector.get_detection(p2)\n",
    "np_l1 = refine_landmarks(l1)\n",
    "np_l2 = refine_landmarks(l2)\n",
    "\n",
    "\n",
    "# 예측 landmark를 통한 평가\n",
    "print(\"normalized result: \")\n",
    "r = evaluate_everything(np_l1, bs1, np_l2, bs2, normalize=True)\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"not normalized result: \")\n",
    "r = evaluate_everything(np_l1, bs1, np_l2, bs2, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Similarity without DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\naver_boostcamp\\project\\level4-cv-finalproject-hackathon-cv-19-lv3\\streamlit\\models\\pose_landmarker_heavy.task 파일이 이미 존재합니다.\n",
      "video information!!\n",
      "FPS:  30\n",
      "total frame length:  845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 845/845 [01:14<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video information!!\n",
      "FPS:  30\n",
      "total frame length:  863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [01:16<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video information!!\n",
      "FPS:  29\n",
      "total frame length:  823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [01:13<00:00, 11.23it/s]\n"
     ]
    }
   ],
   "source": [
    "video_path_1 = \"../videos/마라탕후루1.mp4\"\n",
    "video_path_2 = \"../videos/마라탕후루2.mp4\"\n",
    "wrong_video = \"../videos/케이춤.mp4\"\n",
    "\n",
    "video_detector = PoseDetector(model_size=2, mode='VIDEO')\n",
    "original_frames_1, skeleton_1, ann_1, all_landmarks_1 = video_detector.estimPose_video(video_path_1)\n",
    "video_detector.reset_detector()\n",
    "original_frames_2, skeleton_2, ann_2, all_landmarks_2 = video_detector.estimPose_video(video_path_2)\n",
    "video_detector.reset_detector()\n",
    "_, _, _, wrong_landmarks = video_detector.estimPose_video(wrong_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 본 코드에서는 단순히 짧은 영상에 맞춰서 스코어를 계산하는 로직을 활용할 예정\n",
    "def get_score_from_frames(all_landmarks1, all_landmarks2, score_target='PCK', pck_thres=0.1, thres=0.4, ignore_z=False):\n",
    "    total_results = defaultdict(list)\n",
    "    low_score_frames = []\n",
    "    bs1 = np.array([1, 0, 0])\n",
    "    bs2 = np.array([1, 0, 0])\n",
    "\n",
    "    for frame_num, (landmarks1, landmarks2) in enumerate(zip(all_landmarks1, all_landmarks2)):\n",
    "        np_l1 = refine_landmarks(landmarks1)\n",
    "        np_l2 = refine_landmarks(landmarks2)\n",
    "        results = evaluate_everything(np_l1, bs1, np_l2, bs2, pck_thres=pck_thres, verbose=False, ignore_z=ignore_z)\n",
    "        for k, v in results.items():\n",
    "            total_results[k].append(v)\n",
    "            if score_target in k and results[k] < thres:\n",
    "                low_score_frames.append(frame_num)\n",
    "    \n",
    "    for k in results.keys():\n",
    "        if k==\"matched\":\n",
    "            continue\n",
    "        total_results[k] = np.mean(total_results[k])\n",
    "    \n",
    "    return total_results, low_score_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마라탕후루(28초) vs 마라탕후루(28초)\n",
      "L1_score: 0.9471806770847617\n",
      "L2_distance: 0.32746017876379346\n",
      "cos_similarity: 0.9974753963735945\n",
      "PCK(thres=0.05): 0.6082840236686391\n",
      "oks:: 0.8870574904497737\n",
      "matched: nan\n",
      "low score frame rate:  0.17041420118343195\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"마라탕후루(28초) vs 마라탕후루(28초)\")\n",
    "total_results, low_score_frames = get_score_from_frames(all_landmarks_1, all_landmarks_2, pck_thres=0.05, thres=0.4, ignore_z=True)\n",
    "for k, v in total_results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"low score frame rate: \", len(low_score_frames) / min(len(all_landmarks_1), len(all_landmarks_2)))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마라탕후루(28초) vs 케이춤(27초): \n",
      "L1_score: 0.9321607998192717\n",
      "L2_distance: 0.41872090618002156\n",
      "cos_similarity: 0.9964980324233556\n",
      "PCK(thres=0.05): 0.4621304171729445\n",
      "oks:: 0.8331294437010619\n",
      "matched: nan\n",
      "low score frame rate:  0.362089914945322\n"
     ]
    }
   ],
   "source": [
    "print(\"마라탕후루(28초) vs 케이춤(27초): \")\n",
    "total_results, low_score_frames = get_score_from_frames(all_landmarks_1, wrong_landmarks, pck_thres=0.05, thres=0.4, ignore_z=True)\n",
    "for k, v in total_results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"low score frame rate: \", len(low_score_frames) / min(len(all_landmarks_1), len(wrong_landmarks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Similarity with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'detector' from 'd:\\\\naver_boostcamp\\\\project\\\\level4-cv-finalproject-hackathon-cv-19-lv3\\\\streamlit\\\\detector.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scoring\n",
    "import importlib\n",
    "import detector\n",
    "\n",
    "importlib.reload(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scoring' from 'd:\\\\naver_boostcamp\\\\project\\\\level4-cv-finalproject-hackathon-cv-19-lv3\\\\streamlit\\\\scoring.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "euclidean distance:  973.058483884846\n",
      "cosine similiarity:  813.8465076092934\n",
      "pck:  101.19047619047778\n",
      "\n",
      "\n",
      "wrong\n",
      "euclidean distance:  992.4159004334344\n",
      "cosine similiarity:  791.3661532840445\n",
      "pck:  105.190476190477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from fastdtw import fastdtw\n",
    "from scoring import normalize_landmarks_to_range_by_mean, normalize_landmarks_to_range\n",
    "from copy import deepcopy\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    #return np.inner(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))\n",
    "\n",
    "def pck(a, b, threshold=0.1):\n",
    "    a = a.reshape(-1, 3)\n",
    "    b = b.reshape(-1, 3)\n",
    "    distance = np.linalg.norm(a[:, :3] - b[:, :3], axis=1)\n",
    "    pck_score = np.mean(distance < threshold)\n",
    "    return pck_score\n",
    "\n",
    "def evaluate(all_landmarks_1, all_landmarks_2):\n",
    "    all_landmarks_np_1 = np.array([refine_landmarks(l) for l in all_landmarks_1])\n",
    "    all_landmarks_np_2 = np.array([refine_landmarks(l) for l in all_landmarks_2])\n",
    "    all_landmarks_np_N_2 = normalize_landmarks_to_range_by_mean(all_landmarks_np_1, all_landmarks_np_2)\n",
    "\n",
    "    # flattening to compare\n",
    "    all_landmarks_np_flatten_1 = all_landmarks_np_1[..., :3].reshape(all_landmarks_np_1.shape[0], -1)\n",
    "    all_landmarks_np_flatten_2 = all_landmarks_np_N_2[..., :3].reshape(all_landmarks_np_N_2.shape[0], -1)\n",
    "\n",
    "    distance, path = fastdtw(all_landmarks_np_flatten_1, all_landmarks_np_flatten_2, dist=euclidean)\n",
    "    print(\"euclidean distance: \", distance)\n",
    "\n",
    "    # 유클리디안 거리로 찾아낸 path를 통해 정규화를 다시 수행\n",
    "    all_landmarks_np_flatten_2 = np.zeros_like(all_landmarks_np_2)\n",
    "    for idx1, idx2 in path:\n",
    "        all_landmarks_np_flatten_2[idx2, ...] = normalize_landmarks_to_range(\n",
    "            all_landmarks_np_1[idx1, ...],\n",
    "            all_landmarks_np_N_2[idx2, ...]\n",
    "        )\n",
    "    all_landmarks_np_flatten_2 = all_landmarks_np_flatten_2[..., :3].reshape(all_landmarks_np_2.shape[0], -1)\n",
    "\n",
    "    distance, path = fastdtw(all_landmarks_np_flatten_1, all_landmarks_np_flatten_2, dist=cosine_similarity)\n",
    "    print(\"cosine similiarity: \", distance)\n",
    "\n",
    "    distance, path = fastdtw(all_landmarks_np_flatten_1, all_landmarks_np_flatten_2, dist=pck)\n",
    "    print(\"pck: \", distance)\n",
    "    \n",
    "\n",
    "\n",
    "print('right')\n",
    "evaluate(all_landmarks_1, all_landmarks_2)\n",
    "\n",
    "print('\\n\\nwrong')\n",
    "evaluate(all_landmarks_1, wrong_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skeleton vector활용 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "# select for oks calculation\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "# for cosine similarity\n",
    "vector_list = [\n",
    "    (1, 2),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "    (6, 8),\n",
    "    (9, 11),\n",
    "    (10, 12),\n",
    "    (11, 13),\n",
    "    (12, 14)\n",
    "]\n",
    "\n",
    "\n",
    "# Get keypoints data & bounded box size from 1 frame\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    # return \n",
    "    # keypoints : list([x, y, z, visibility], ...)\n",
    "    # boxsize : detection box length[가로, 세로, 높이]\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "            \n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin, landmark.y), max(ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "    \n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# Calculate OKS value from 2 keypoints data from each data\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c**2)))\n",
    "\n",
    "\n",
    "# Make cosine similarity to percent form\n",
    "def cosine_similarity_to_percentage(similarity_list):\n",
    "    similarity = np.mean(similarity_list)\n",
    "    return (similarity + 1) * 50\n",
    "\n",
    "\n",
    "# Calculate cosine similarity from each keypoint data\n",
    "def cos_sim_w_keypoint(keypoints1, keypoints2):\n",
    "    global vector_list\n",
    "    cos_sim_list = []\n",
    "\n",
    "    for vector in vector_list:\n",
    "        z_num = 2\n",
    "        idx1, idx2 = vector\n",
    "        vec1 = (keypoints1[idx2][:z_num] - keypoints1[idx1][:z_num]).reshape(1, -1)\n",
    "        vec2 = (keypoints2[idx2][:z_num] - keypoints2[idx1][:z_num]).reshape(1, -1)\n",
    "        sim_value = cosine_similarity(vec1, vec2)\n",
    "        cos_sim_list.append(sim_value)\n",
    "    \n",
    "    return cos_sim_list\n",
    "\n",
    "\n",
    "# Calculate OKS & Cosine similarity from each keypoint data\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "    keypoints1 = np.array(keypoints1)\n",
    "    keypoints2 = np.array(keypoints2)\n",
    "\n",
    "    if keypoints1.shape != keypoints2.shape:\n",
    "        print(keypoints1.shape, keypoints2.shape)\n",
    "        raise ValueError(\"Keypoint shapes do not match!\")\n",
    "    \n",
    "    oks_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "\n",
    "    cos_sim_list = cos_sim_w_keypoint(keypoints1, keypoints2)\n",
    "\n",
    "    return cosine_similarity_to_percentage(np.mean(cos_sim_list)), (np.mean(oks_list)) * 100\n",
    "\n",
    "\n",
    "# Make mean coordinate data from keypoints list\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    frame_count = -1\n",
    "\n",
    "    # List of OKS & Cosine similarity from each frame\n",
    "    okslist = []\n",
    "    cos_list = []\n",
    "\n",
    "    # List of OKS & Cosine similarity from every 15 frame\n",
    "    okslist_mean = []\n",
    "    cos_list_mean = []\n",
    "\n",
    "    # Make keypoint list\n",
    "    list_keypoints1 = []\n",
    "    list_keypoints2 = []\n",
    "\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        frame_count += 1\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            keypoints1, boxsize = get_keypoints_and_boxsize(frame1)\n",
    "            keypoints2, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            list_keypoints1.append(keypoints1)\n",
    "            list_keypoints2.append(keypoints2)\n",
    "\n",
    "            similarity, oks_percent = weighted_similarity(keypoints1, keypoints2, boxsize) # Calculate Scores from each frame\n",
    "            okslist.append(oks_percent)\n",
    "            cos_list.append(similarity)\n",
    "            print(f\"Frame {frame_count+1}: Weighted similarity between keypoints1 and video: {similarity}\")\n",
    "            print(f\"Frame {frame_count+1}: Weighted similarity between keypoints1 and video: {oks_percent}\")\n",
    "\n",
    "            if len(list_keypoints1) == 15:\n",
    "                mean_keypoints1 = mean_value_of_keypoints(list_keypoints1)\n",
    "                mean_keypoints2 = mean_value_of_keypoints(list_keypoints2)\n",
    "\n",
    "                similarity_mean, oks_percent_mean = weighted_similarity(mean_keypoints1, mean_keypoints2, boxsize) # Calculate Scores from each mean frame\n",
    "                okslist_mean.append(oks_percent_mean)\n",
    "                cos_list_mean.append(similarity_mean)\n",
    "                print(f\"Frame {frame_count+1}: Weighted similarity between mean keypoints1 and video: {similarity_mean}\")\n",
    "                print(f\"Frame {frame_count+1}: Weighted similarity between mean keypoints1 and video: {oks_percent_mean}\")\n",
    "\n",
    "                list_keypoints1 = []\n",
    "                list_keypoints2 = []\n",
    "            \n",
    "            # Press 'q' to exit the loop and close the video window\n",
    "            if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f'oks = {np.mean(okslist)}, cos = {np.mean(cos_list)}')           # Print the score from each frame\n",
    "    print(f'oks = {np.mean(okslist_mean)}, cos = {np.mean(cos_list_mean)}') # Print the score from every 15 frame\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../엔터테이먼트.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKS(거리기반 스코어), PCK(거리기반 정확도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Mediapipe 라이브러리를 임포트합니다.\n",
    "import numpy as np # 배열 및 수학 연산을 위한 NumPy 라이브러리를 임포트합니다.\n",
    "import cv2 # OpenCV 라이브러리를 임포트합니다.\n",
    "import shutil # 파일 복사 및 삭제를 위한 shutil 모듈을 임포트합니다.\n",
    "import os # 운영 체제 관련 작업을 위한 os 모듈을 임포트합니다.\n",
    "import json # JSON 데이터 처리를 위한 모듈을 임포트합니다.\n",
    "\n",
    "\n",
    "# Mediapipe 라이브러리를 이용한 포즈 추출 설정\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 사용할 키포인트 이름 및 선택된 키포인트 인덱스 목록 설정\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "oks_cnt = [[] for _ in range(11)]\n",
    "pck_cnt = [[] for _ in range(11)]\n",
    "\n",
    "def delete_file_or_folder(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                print(f\"File {path} deleted successfully.\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"Folder {path} and its contents deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Path {path} not found. Skipping deletion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting: {e}\")\n",
    "\n",
    "\n",
    "# 정답 프레임에서 키포인트 데이터 및 바운딩 박스 크기를 가져오는 함수\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin,\n",
    "                                                                                                       landmark.y), max(\n",
    "                    ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "\n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# OKS 값 계산 함수\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c ** 2)))\n",
    "\n",
    "\n",
    "# PCK 값 계산 함수\n",
    "def pck(gt, preds, threshold):\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    return 1.0 if distance < threshold else 0.0\n",
    "\n",
    "\n",
    "# 가중치가 적용된 유사도 계산 함수\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "        pck_list.append(pck(keypoints1[i][:3], keypoints2[i][:3], 0.1))\n",
    "\n",
    "    return (np.mean(oks_list)) * 100, (np.mean(pck_list)) * 100\n",
    "\n",
    "\n",
    "# 키포인트 리스트의 평균 좌표 계산 함수\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    frame_cnt = 0\n",
    "    # 업로드된 동영상 파일을 열기\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    # 각 프레임에서의 OKS 및 pck의 리스트\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "\n",
    "    # 사용자의 키포인트 리스트\n",
    "    user_keypoints = []\n",
    "\n",
    "    # 동영상의 모든 프레임을 처리\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        ret1, frame1 = cap1.read()\n",
    "        frame1 = cv2.flip(frame1, 1)\n",
    "\n",
    "        ret2, frame2 = cap2.read()\n",
    "        frame2 = cv2.flip(frame2, 1)\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            # 현재 프레임에서 사용자의 키포인트 및 바운딩 박스 크기 가져오기\n",
    "            user_key, _ = get_keypoints_and_boxsize(frame1)\n",
    "            user_keypoints.append(user_key)\n",
    "\n",
    "            answer_key, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            # 만약 정답 키포인트와 사용자 키포인트가 존재하면 점수 계산\n",
    "            if len(answer_key) > 0 and len(user_key) > 0:\n",
    "                oks_percent, pck_percent = weighted_similarity(np.array(answer_key), np.array(user_key),\n",
    "                                                               _)  # Calculate Scores from each frame\n",
    "                oks_cnt[int(oks_percent / 10)].append(frame_cnt)\n",
    "                pck_cnt[int(pck_percent / 10)].append(frame_cnt)\n",
    "\n",
    "                oks_list.append(oks_percent)\n",
    "                pck_list.append(pck_percent)\n",
    "        else:\n",
    "            break\n",
    "        frame_cnt = frame_cnt + 1\n",
    "\n",
    "    oks_answer = np.mean(oks_list)\n",
    "    pck_answer = np.mean(pck_list)\n",
    "    print(\"oks =\", oks_answer, \"pck =\", pck_answer)\n",
    "\n",
    "    # JSON 응답에 넣을 데이터를 딕셔너리로 만듦 (값들을 float로 변환)\n",
    "    response_data = {\n",
    "        \"oks_30\": oks_answer,\n",
    "        \"pck_30\": pck_answer,\n",
    "        \"oks_frame_score\": oks_list,\n",
    "        \"pck_frame_score\": pck_list\n",
    "    }\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks, _ = get_keypoints_and_boxsize(cv2.imread('../images/right_pose_img.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(landmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../마라탕후루2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루1.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(\"../마라탕후루2.mp4\", \"../엔터테이먼트.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거리기반 OKS, PCK and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Mediapipe 라이브러리를 임포트합니다.\n",
    "import numpy as np # 배열 및 수학 연산을 위한 NumPy 라이브러리를 임포트합니다.\n",
    "import cv2 # OpenCV 라이브러리를 임포트합니다.\n",
    "import shutil # 파일 복사 및 삭제를 위한 shutil 모듈을 임포트합니다.\n",
    "import os # 운영 체제 관련 작업을 위한 os 모듈을 임포트합니다.\n",
    "import json # JSON 데이터 처리를 위한 모듈을 임포트합니다.\n",
    "\n",
    "\n",
    "# Mediapipe 라이브러리를 이용한 포즈 추출 설정\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 사용할 키포인트 이름 및 선택된 키포인트 인덱스 목록 설정\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "    \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "    \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "    \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "    \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\",\n",
    "    \"left knee\", \"right knee\", \"left ankle\", \"right ankle\", \"left heel\",\n",
    "    \"right heel\", \"left foot index\", \"right foot index\"\n",
    "]\n",
    "\n",
    "selected_keypoints = [0,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n",
    "\n",
    "connections = [\n",
    "    (0,1), (0,2), # Nose to Ears\n",
    "    (3,5), (4,6), # Shoulders to Elbows\n",
    "    (5,7), (6,8), # Elbows to Wrists\n",
    "    (9,11), (10,12), # Hips to Knees\n",
    "    (11,13), (12,14), # Knees to Ankles\n",
    "    (3, 4), (4, 10), (10, 9), (9, 3) # body\n",
    "]\n",
    "\n",
    "oks_cnt = [[] for _ in range(11)]\n",
    "pck_cnt = [[] for _ in range(11)]\n",
    "\n",
    "def delete_file_or_folder(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "                print(f\"File {path} deleted successfully.\")\n",
    "            elif os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"Folder {path} and its contents deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Path {path} not found. Skipping deletion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting: {e}\")\n",
    "\n",
    "\n",
    "# 정답 프레임에서 키포인트 데이터 및 바운딩 박스 크기를 가져오는 함수\n",
    "def get_keypoints_and_boxsize(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in selected_keypoints:\n",
    "                keypoints.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = 0, 0, 0, 0, 0, 0\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            if xmin == 0:\n",
    "                xmin, ymin, zmin = landmark.x, landmark.y, landmark.z\n",
    "\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax, zmin, zmax = min(xmin, landmark.x), max(xmax, landmark.x), min(ymin,\n",
    "                                                                                                       landmark.y), max(\n",
    "                    ymax, landmark.y), min(zmin, landmark.z), max(zmax, landmark.z)\n",
    "\n",
    "    boxsize = (xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "    boxsize = [boxsize[2 * i + 1] - boxsize[2 * i] for i in range(3)]\n",
    "\n",
    "    return keypoints, boxsize\n",
    "\n",
    "\n",
    "# OKS 값 계산 함수\n",
    "def oks(gt, preds, idx, boxsize):\n",
    "    sigmas = np.array([.026, .035, .035, .079, .079, .072, .072, .062, .062, .107, .107, .087, .087, .089, .089])\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    bbox_gt = boxsize[0] ** 2 + boxsize[1] ** 2\n",
    "    kp_c = sigmas[idx]\n",
    "    return np.exp(-(dx ** 2 + dy ** 2) / (2 * (bbox_gt) * (kp_c ** 2)))\n",
    "\n",
    "\n",
    "# PCK 값 계산 함수\n",
    "def pck(gt, preds, threshold):\n",
    "    dx = gt[0] - preds[0]\n",
    "    dy = gt[1] - preds[1]\n",
    "    distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    return 1.0 if distance < threshold else 0.0\n",
    "\n",
    "\n",
    "# 가중치가 적용된 유사도 계산 함수\n",
    "def weighted_similarity(keypoints1, keypoints2, boxsize):\n",
    "\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "    for i in range(len(keypoints1)):\n",
    "        oks_list.append(oks(keypoints1[i][:3], keypoints2[i][:3], i, boxsize))\n",
    "        pck_list.append(pck(keypoints1[i][:3], keypoints2[i][:3], 0.1))\n",
    "\n",
    "    return (np.mean(oks_list)) * 100, (np.mean(pck_list)) * 100\n",
    "\n",
    "\n",
    "# 키포인트 리스트의 평균 좌표 계산 함수\n",
    "def mean_value_of_keypoints(keypoints):\n",
    "    mean_of_keypoints = np.zeros_like(keypoints[0])\n",
    "    for key in keypoints:\n",
    "        mean_of_keypoints += key\n",
    "\n",
    "    mean_of_keypoints /= len(keypoints)\n",
    "    return mean_of_keypoints\n",
    "\n",
    "\n",
    "def Scoring(video_path1, video_path2):\n",
    "    frame_cnt = 0\n",
    "    # 업로드된 동영상 파일을 열기\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    # 각 프레임에서의 OKS 및 pck의 리스트\n",
    "    oks_list = []\n",
    "    pck_list = []\n",
    "\n",
    "    # 사용자의 키포인트 리스트\n",
    "    user_keypoints = []\n",
    "\n",
    "    # 동영상의 모든 프레임을 처리\n",
    "    while cap1.isOpened() and cap2.isOpened():\n",
    "        ret1, frame1 = cap1.read()\n",
    "        frame1 = cv2.flip(frame1, 1)\n",
    "\n",
    "        ret2, frame2 = cap2.read()\n",
    "        frame2 = cv2.flip(frame2, 1)\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            # 현재 프레임에서 사용자의 키포인트 및 바운딩 박스 크기 가져오기\n",
    "            user_key, _ = get_keypoints_and_boxsize(frame1)\n",
    "            user_keypoints.append(user_key)\n",
    "\n",
    "            answer_key, _ = get_keypoints_and_boxsize(frame2)\n",
    "\n",
    "            # 만약 정답 키포인트와 사용자 키포인트가 존재하면 점수 계산\n",
    "            if len(answer_key) > 0 and len(user_key) > 0:\n",
    "                oks_percent, pck_percent = weighted_similarity(np.array(answer_key), np.array(user_key),\n",
    "                                                               _)  # Calculate Scores from each frame\n",
    "                oks_cnt[int(oks_percent / 10)].append(frame_cnt)\n",
    "                pck_cnt[int(pck_percent / 10)].append(frame_cnt)\n",
    "\n",
    "                oks_list.append(oks_percent)\n",
    "                pck_list.append(pck_percent)\n",
    "        else:\n",
    "            break\n",
    "        frame_cnt = frame_cnt + 1\n",
    "\n",
    "    oks_answer = np.mean(oks_list)\n",
    "    pck_answer = np.mean(pck_list)\n",
    "    print(\"oks =\", oks_answer, \"pck =\", pck_answer)\n",
    "\n",
    "    # JSON 응답에 넣을 데이터를 딕셔너리로 만듦 (값들을 float로 변환)\n",
    "    response_data = {\n",
    "        \"oks_30\": oks_answer,\n",
    "        \"pck_30\": pck_answer,\n",
    "        \"oks_frame_score\": oks_list,\n",
    "        \"pck_frame_score\": pck_list\n",
    "    }\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe_inference import get_detection\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scoring import refine_landmarks\n",
    "\n",
    "\n",
    "def normalize_landmarks_to_range(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Normalize landmarks2 to match the coordinate range of landmarks1.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (numpy array): Keypoints array for the first pose (num_selected_point, 4).\n",
    "        landmarks2 (numpy array): Keypoints array for the second pose (num_selected_point, 4).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Normalized landmarks2 matching the range of landmarks1.\n",
    "    \"\"\"\n",
    "    # Calculate min and max for landmarks1 and landmarks2\n",
    "    min1 = np.min(landmarks1[:, :3], axis=0)  # (x_min, y_min, z_min) for landmarks1\n",
    "    max1 = np.max(landmarks1[:, :3], axis=0)  # (x_max, y_max, z_max) for landmarks1\n",
    "\n",
    "    min2 = np.min(landmarks2[:, :3], axis=0)  # (x_min, y_min, z_min) for landmarks2\n",
    "    max2 = np.max(landmarks2[:, :3], axis=0)  # (x_max, y_max, z_max) for landmarks2\n",
    "\n",
    "    # Normalize landmarks2 to the range of landmarks1\n",
    "    normalized_landmarks2 = (landmarks2[:, :3] - min2) / (max2 - min2) * (max1 - min1) + min1\n",
    "\n",
    "    # Combine normalized coordinates with the original visibility values\n",
    "    normalized_landmarks2 = np.hstack((normalized_landmarks2, landmarks2[:, 3:4]))\n",
    "\n",
    "    return normalized_landmarks2\n",
    "\n",
    "\n",
    "def draw_landmarks_on_blank_image(landmarks1, landmarks2, image_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Draw two sets of landmarks on a blank image.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (numpy array): Keypoints array for the first pose (num_selected_point, 4).\n",
    "        landmarks2 (numpy array): Keypoints array for the second pose (num_selected_point, 4).\n",
    "        image_size (tuple): Size of the blank image (height, width).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Image with landmarks visualized.\n",
    "    \"\"\"\n",
    "    # Create a blank image\n",
    "    blank_image = np.zeros((image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Normalize coordinates to fit within the image size\n",
    "    def normalize_coordinates(landmarks, image_size):\n",
    "        height, width = image_size\n",
    "        normalized_landmarks = []\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark[0] * width), int(landmark[1] * height)\n",
    "            normalized_landmarks.append((x, y))\n",
    "        return normalized_landmarks\n",
    "\n",
    "    # Normalize landmarks\n",
    "    normalized_landmarks1 = normalize_coordinates(landmarks1, image_size)\n",
    "    normalized_landmarks2 = normalize_coordinates(landmarks2, image_size)\n",
    "\n",
    "    # Draw landmarks1 (red)\n",
    "    for x, y in normalized_landmarks1:\n",
    "        cv2.circle(blank_image, (x, y), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    # Draw landmarks2 (blue)\n",
    "    for x, y in normalized_landmarks2:\n",
    "        cv2.circle(blank_image, (x, y), radius=5, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "    # Optionally connect landmarks (example: skeletal connections can be added here)\n",
    "    # You can define a list of connections (e.g., POSE_CONNECTIONS) to draw lines between keypoints.\n",
    "\n",
    "    return blank_image\n",
    "\n",
    "p1 = \"../images/승윤팔짱1.jpg\"\n",
    "p2= \"../images/준일팔짱.jpg\"\n",
    "l1, seg1, ann_img1, bs1 = get_detection(p1)\n",
    "l2, seg2, ann_img2, bs2 = get_detection(p2)\n",
    "np_l1 = refine_landmarks(l1)\n",
    "np_l2 = refine_landmarks(l2)\n",
    "np_l2 = normalize_landmarks_to_range(np_l1, np_l2)\n",
    "\n",
    "from keypoint_map import KEYPOINT_MAPPING, SELECTED_KEYPOINTS\n",
    "for i in range(np_l1.shape[0]):\n",
    "    print(f\"difference of keypoint {KEYPOINT_MAPPING[SELECTED_KEYPOINTS[i]]}: {np.linalg.norm(np_l2[i] - np_l1[i])}\")\n",
    "\n",
    "plt.imshow(draw_landmarks_on_blank_image(np_l1, np_l2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
